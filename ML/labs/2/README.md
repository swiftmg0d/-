**_Лабораториска Вежба 2_**

**Линеарна Регресија, Lasso Регресија и Ridge Регресија**

Целта на оваа лабораториска вежба е да ги тестирате регресионите модели кои ги имаме учено на California Housing податочното множество. За решавање на истата би требало да ви помогнат примерите кои се разгледани за време на часовите.

_Што точно треба да направите?_

1. **Избирање на податочно множество**. Се препорачува да изберете некое податочно множество од **sklearn.datasets** кое може директно да го вчитате со готова наредба од таму. Внимавајте избраното податочно множество да е соодветно за проблемот за да е целта да се предвиди вредноста на излезната променлива.

Слободно може да користите и друго регресионо податочно множество доколку имате или доколку сакате да најдете на UC Irvine archive или на Kaggle.

1. **Запознавање со податочното множество и претпроцесирање**. Во овој дел повторно би требало да направите дел од визуелизациите на вашето податочно множество за да имате некоја претстава за атрибутите во однос на таргет променливата. За таа цел може да тестирате различни верзии на jointplot функцијата (<https://seaborn.pydata.org/generated/seaborn.jointplot.html>) од seaborn библиотеката. Можете слободно да направите и други визуелизации кои би ги нашле на страниците на seaborn и matplotlib. Дополнително направете min-max нормализација на податоците во опсег \[0,1\].
2. **Регресија**. Истренирајте повеќе регресиони модели користејќи ја библиотеката Scikit-Learn на податочното множество што сте го избрале.
   - LinearRegression
   - Ridge
   - Lasso
   - ElasticNet
   - BayesianRidge

Како што е направено во примерот, прво поделете го оригиналното податочно множество на тренинг и тест множество и истренирајте го регресиониот модел на тренинг множеството. Притоа може да видите дека за разлика од примерите од часовите каде што ги користевме стандардните Lasso и Ridge класификатори од Scikit-Learn, во оваа лабораториска треба да се искористат LassoCV и RidgeCV класификаторите. Целта на овие класи (и разликата помеѓу стандардните верзии) е тоа што во нив е веќе изграден метод на cross-validation кој на самото тренинг множество го оптимизира хипер-параметарот на овие модели (alpha во scikit-learn, или lambda во книгата). Овој хипер-параметар служи за ниво на регуларизација во двата модели и поголеми вредности соодветствуваат на поголема регуларизација. Со цел сами да не го избираме овој хипер-параметар, со овие класи можеме да го најдеме тој што ќе има најсоодветна вредност во однос на податоците кои ги тренираме. Откако ќе ги истренирате регресорите, пресметајте и споредете го MSE (Mean Square Error) на тест множеството за секој од нив, приметете дали има некоја разлика во коефициентите на моделите, како и која е соодветната вредност за хипер-параметарот на регуларизација во Lasso и Ridge.

1. **Документација:**
   - На курсот поставете го .ipynb фајлот со решенијата.
